{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2582,"status":"ok","timestamp":1650234126912,"user":{"displayName":"田泽睿","userId":"02898357169809669043"},"user_tz":240},"id":"nybV8i1qYcnV","outputId":"4cd18230-0c6b-4fa6-cb71-a279544fc612"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"7zgbklteKmsU","executionInfo":{"status":"ok","timestamp":1650235359729,"user_tz":240,"elapsed":4,"user":{"displayName":"田泽睿","userId":"02898357169809669043"}}},"outputs":[],"source":["import collections\n","import os\n","import random\n","import torch\n","import numpy as np\n","import time\n","import torch.utils.data as Data\n","from torch import nn\n","from tqdm import tqdm\n","from collections import Counter\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import vocab"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"aZt5-x5h-1Dm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b50a4117-102a-4f9d-8938-ac8afdf06b30","executionInfo":{"status":"ok","timestamp":1650234843955,"user_tz":240,"elapsed":317948,"user":{"displayName":"田泽睿","userId":"02898357169809669043"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 12500/12500 [01:32<00:00, 135.58it/s] \n","100%|██████████| 12500/12500 [00:14<00:00, 863.90it/s] \n","100%|██████████| 12500/12500 [00:14<00:00, 858.74it/s] \n","100%|██████████| 12500/12500 [03:15<00:00, 63.88it/s]  \n"]}],"source":["# Read the data\n","\n","def read_data(folder='train', data_root='/content/drive/MyDrive/'):\n","    data = []\n","    for label in ['pos', 'neg']:\n","        folder_name = os.path.join(data_root, folder, label)\n","        for file in tqdm(os.listdir(folder_name)):\n","            with open(os.path.join(folder_name, file), 'rb') as f:\n","                review = f.read().decode('utf-8').replace('\\n', '').lower()\n","                data.append([review, 1 if label == 'pos' else 0])\n","    random.shuffle(data)\n","    return data\n","\n","DATA_ROOT = '/content/drive/MyDrive'\n","data_root = os.path.join(DATA_ROOT, \"aclImdb\")\n","train_data, test_data = read_data('train', data_root), read_data('test', data_root)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"Q7z_sicA_vu5","executionInfo":{"status":"ok","timestamp":1650235757658,"user_tz":240,"elapsed":2823,"user":{"displayName":"田泽睿","userId":"02898357169809669043"}}},"outputs":[],"source":["# build the function we need in the next steps \n","\n","def tokenizer(text):\n","    return [tok.lower() for tok in text.split(' ')]\n","\n","def get_tokenized_data(data):\n","    return [tokenizer(review) for review, _ in data]\n","\n","def get_vocab_data(data):\n","    tokenized_data = get_tokenized_data(data)\n","    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n","    return vocab(counter,min_freq=10)\n","\n","vocab = get_vocab_data(train_data) # This is used for filter some words have low frequency"]},{"cell_type":"code","source":["# calculate TD-IDF results\n","\n","def TF_IDF_trans(word,counter1,counter2):\n","  TF1 = 0\n","  TF2 = 0\n","  flag1 = 0\n","  flag2 = 0 \n","  TF1 = counter1[word]\n","  TF2 = counter2[word]\n","  flag1 = 1 if TF1!=0 else 0\n","  flag2 = 1 if TF2!=0 else 0\n","  IDF = np.log((2+1)/(flag1+flag2+1))+1\n","  return [TF1*IDF,TF2*IDF]"],"metadata":{"id":"gHsQhtNAx35R","executionInfo":{"status":"ok","timestamp":1650235183741,"user_tz":240,"elapsed":232,"user":{"displayName":"田泽睿","userId":"02898357169809669043"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["cato1 = [train_data[i] for i in range(len(train_data)) if train_data[i][1] == 1]\n","cato2 = [train_data[i] for i in range(len(train_data)) if train_data[i][1] == 0]\n","cato1 = get_tokenized_data(cato1)\n","cato2 = get_tokenized_data(cato2)\n","counter1 = collections.Counter([tk for st in cato1 for tk in st])  #used to calculate TF-IDF\n","counter2 = collections.Counter([tk for st in cato2 for tk in st])"],"metadata":{"id":"wMrTlGlQ6sN6","executionInfo":{"status":"ok","timestamp":1650235188236,"user_tz":240,"elapsed":2431,"user":{"displayName":"田泽睿","userId":"02898357169809669043"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["tokenized_data = get_tokenized_data(train_data)\n","embedding = {} # record the TD-IDF results\n","for words in tokenized_data:\n","  for word in words:\n","    if word not in embedding:\n","      embedding[word] = TF_IDF_trans(word,counter1,counter2)"],"metadata":{"id":"DBZ2dvwbxPkD","executionInfo":{"status":"ok","timestamp":1650235207553,"user_tz":240,"elapsed":4229,"user":{"displayName":"田泽睿","userId":"02898357169809669043"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Preprocess the data\n","\n","def preprocess_data(data, embedding):\n","  tokenized_data = get_tokenized_data(data)\n","  max_l = 500 \n","  def pad(x):\n","    return x[:max_l] if len(x) > max_l else x + [[0,0]] * (max_l -len(x))\n","  features = torch.tensor([pad([embedding[word] for word in words if word in embedding and word in vocab]) for words in tokenized_data])\n","  labels = torch.tensor([score for _, score in data])\n","  return features, labels"],"metadata":{"id":"rvRByS_jF4zF","executionInfo":{"status":"ok","timestamp":1650235769071,"user_tz":240,"elapsed":200,"user":{"displayName":"田泽睿","userId":"02898357169809669043"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","execution_count":31,"metadata":{"id":"DQvCZ81IAoJu","executionInfo":{"status":"ok","timestamp":1650235792208,"user_tz":240,"elapsed":21982,"user":{"displayName":"田泽睿","userId":"02898357169809669043"}}},"outputs":[],"source":["# Create data iterator\n","\n","batch_size = 64\n","train_set = Data.TensorDataset(*preprocess_data(train_data, embedding))\n","test_set = Data.TensorDataset(*preprocess_data(test_data, embedding))\n","train_iter = Data.DataLoader(train_set, batch_size, shuffle=True)\n","test_iter = Data.DataLoader(test_set, batch_size)"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227,"status":"ok","timestamp":1650237541988,"user":{"displayName":"田泽睿","userId":"02898357169809669043"},"user_tz":240},"id":"u_L3bxdiAqPk","outputId":"ffe333ae-e212-4c1e-f0ac-5eb0c8b7c236"},"outputs":[{"output_type":"stream","name":"stdout","text":["X torch.Size([64, 500, 2]) y torch.Size([64])\n"]}],"source":["# check the data\n","\n","for X, y in train_iter:\n","  print('X', X.shape, 'y', y.shape)\n","  break"]},{"cell_type":"code","source":["# Create Bi-LSTM model\n","\n","class BiRNN(nn.Module):\n","  def __init__(self, embed_size, num_hiddens, num_layers):\n","    super(BiRNN, self).__init__()\n","    self.LSTM = nn.LSTM(input_size=embed_size,hidden_size=num_hiddens,num_layers=num_layers,bidirectional=True)\n","    self.fc = nn.Linear(4*num_hiddens, 2)\n","    self.dropout = nn.Dropout(0.8)\n","  def forward(self, inputs):\n","    inputs = inputs.float()\n","    outputs, _ = self.LSTM(inputs.permute(1,0,2))\n","    outputs = self.dropout(outputs)\n","    result = torch.cat((outputs[0], outputs[-1]), -1)\n","    outs = self.fc(result)\n","    return outs "],"metadata":{"id":"NyiiDdIQJUym","executionInfo":{"status":"ok","timestamp":1650236361900,"user_tz":240,"elapsed":207,"user":{"displayName":"田泽睿","userId":"02898357169809669043"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","execution_count":39,"metadata":{"id":"LoMk8Z4_Z7-q","executionInfo":{"status":"ok","timestamp":1650236362300,"user_tz":240,"elapsed":210,"user":{"displayName":"田泽睿","userId":"02898357169809669043"}}},"outputs":[],"source":["# Parameters we need\n","\n","embed_size, num_hiddens, num_layers = 2, 100, 2\n","net = BiRNN(embed_size, num_hiddens, num_layers)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","lr, num_epochs = 0.001, 5\n","optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n","loss = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"wOQGjdXyb4wi","executionInfo":{"status":"ok","timestamp":1650236363844,"user_tz":240,"elapsed":217,"user":{"displayName":"田泽睿","userId":"02898357169809669043"}}},"outputs":[],"source":["def evaluate_accuracy(data_iter, net,device):\n","  acc_sum, n = 0.0, 0\n","  with torch.no_grad():\n","    for X, y in data_iter:\n","      net.eval()\n","      acc_sum += (net(X.to(device)).argmax(dim=1) ==y.to(device)).float().sum().cpu().item()\n","      net.train()\n","      n += y.shape[0]\n","  return acc_sum / n"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"78dozb9MaL1C","executionInfo":{"status":"ok","timestamp":1650237134244,"user_tz":240,"elapsed":208,"user":{"displayName":"田泽睿","userId":"02898357169809669043"}}},"outputs":[],"source":["def train(train_iter, test_iter, net, loss, optimizer, device,num_epochs):\n","  net = net.to(device)\n","  batch_count = 0\n","  for epoch in range(num_epochs):\n","    train_loss_sum, train_acc_sum, n = 0.0, 0.0, 0\n","    for X, y in train_iter: \n","      optimizer.zero_grad()\n","      X = X.to(device) \n","      y = y.to(device)\n","      y_hat = net(X) \n","      l = loss(y_hat, y)\n","      l.backward()\n","      optimizer.step()\n","      train_loss_sum += l.cpu().item()\n","      train_acc_sum += (y_hat.argmax(dim=1) ==y).sum().cpu().item()\n","      n += y.shape[0]\n","      batch_count += 1\n","    test_acc = evaluate_accuracy(test_iter, net,device)\n","    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'% (epoch + 1, train_loss_sum / batch_count,train_acc_sum / n, test_acc))"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":292676,"status":"ok","timestamp":1650237430597,"user":{"displayName":"田泽睿","userId":"02898357169809669043"},"user_tz":240},"id":"vyCYEQBmbnK8","outputId":"0face8eb-6ddb-4c07-eecd-8b1fa09871f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1, loss 0.2145, train acc 0.916, test acc 0.815\n","epoch 2, loss 0.1056, train acc 0.917, test acc 0.830\n","epoch 3, loss 0.0701, train acc 0.917, test acc 0.836\n","epoch 4, loss 0.0523, train acc 0.917, test acc 0.837\n","epoch 5, loss 0.0418, train acc 0.917, test acc 0.838\n"]}],"source":["train(train_iter, test_iter, net, loss, optimizer, device,num_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uLuPNSJ98u2f"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"DL4H_project.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}